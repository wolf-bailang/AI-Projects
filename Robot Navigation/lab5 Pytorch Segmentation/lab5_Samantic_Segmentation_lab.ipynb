{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab5-Samantic_Segmentation_lab.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwfKhFfHF27m",
        "colab_type": "text"
      },
      "source": [
        "# Semantic Segmentation with PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGeguvkAGNlw",
        "colab_type": "text"
      },
      "source": [
        "Mount google drive to colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHc49hWDOJKw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWNmktpFOddS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if you mount Google drive correctly, the following commands should be able to executed correctly\n",
        "!ls /content/drive/\n",
        "%cd \"/content/drive/My Drive/CamVid\"\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jly0ouf9eNOQ",
        "colab_type": "text"
      },
      "source": [
        "Import neccessary libraties and set parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qzie3tp6QThn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import models\n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "\n",
        "from PIL import Image\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyGTWuMwngPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset path\n",
        "root_dir   = \"/content/drive/My Drive/CamVid/\"\n",
        "train_file = os.path.join(root_dir, \"train.csv\")\n",
        "val_file   = os.path.join(root_dir, \"val.csv\")\n",
        "\n",
        "print(\"training csv exits:{}\".format(os.path.exists(train_file)))\n",
        "print(\"validation csv exits:{}\".format(os.path.exists(val_file)))\n",
        "\n",
        "# Create folder to store training results.\n",
        "val_dir = \"/content/drive/My Drive/segmentation_output/\"\n",
        "if os.path.isdir(val_dir) == False:\n",
        "   os.mkdir(val_dir)\n",
        "\n",
        "# Parameters\n",
        "num_class = 11 # 32 for original CamVid\n",
        "input_h, input_w = 256, 256\n",
        "batch_size = 16\n",
        "epochs = 40\n",
        "lr = 1e-4\n",
        "use_gpu = torch.cuda.is_available()\n",
        "\n",
        "# index for validation images\n",
        "global_index = 0\n",
        "\n",
        "# pixel accuracy and mIOU list \n",
        "pixel_acc_list = []\n",
        "mIOU_list = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nphF6DjpeZVB",
        "colab_type": "text"
      },
      "source": [
        "## CamVid Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEgjfY74izJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CamVidDataset(Dataset):\n",
        "    def __init__(self, csv_file, n_class=num_class, flip_rate=0.5, rand_crop=True):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.n_class = n_class\n",
        "        self.new_h = input_h\n",
        "        self.new_w = input_w\n",
        "        self.flip_rate = flip_rate  \n",
        "        self.rand_crop = rand_crop\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # open image data\n",
        "        img_name   = self.data.iloc[idx, 0]                \n",
        "        img_name = root_dir  + img_name                        \n",
        "        img = Image.open(img_name).convert('RGB')\n",
        "        \n",
        "        # open label data\n",
        "        label_name = self.data.iloc[idx, 1]        \n",
        "        label_name = root_dir  + label_name                       \n",
        "        label_image = Image.open(label_name)\n",
        "        \n",
        "        # crop images and labels\n",
        "        w, h = img.size\n",
        "        if self.rand_crop:            \n",
        "            A_x_offset = np.int32(np.random.randint(0, w - self.new_w + 1, 1))[0]\n",
        "            A_y_offset = np.int32(np.random.randint(0, h - self.new_h + 1, 1))[0]\n",
        "        else:            \n",
        "            A_x_offset = int((w - self.new_w)/2)\n",
        "            A_y_offset = int((h - self.new_h)/2)\n",
        "       \n",
        "        img = img.crop((A_x_offset, A_y_offset, A_x_offset + self.new_w, A_y_offset + self.new_h)) # left, top, right, bottom\n",
        "        label_image = label_image.crop((A_x_offset, A_y_offset, A_x_offset + self.new_w, A_y_offset + self.new_h)) # left, top, right, bottom\n",
        "\n",
        "        # flip images and labels\n",
        "        img = np.transpose(img, (2, 0, 1)) / 255.\n",
        "        label = np.asarray(label_image)\n",
        "        if np.random.sample() < self.flip_rate:\n",
        "            img = np.fliplr(img)\n",
        "            label = np.fliplr(label)\n",
        "\n",
        "        # create tensor\n",
        "        img = torch.from_numpy(img.copy()).float()\n",
        "        label = torch.from_numpy(label.copy()).long()\n",
        "\n",
        "        # create one-hot encoding tensor\n",
        "        h, w = label.size()\n",
        "        target = torch.zeros(self.n_class, h, w)\n",
        "        for c in range(self.n_class):\n",
        "            target[c][label == c] = 1\n",
        "\n",
        "        sample = {'X': img, 'Y': target, 'l': label}\n",
        "        return sample\n",
        "\n",
        "# Load dataset\n",
        "train_data = CamVidDataset(csv_file=train_file, flip_rate=0.5, rand_crop=True)\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, num_workers=8)\n",
        "val_data = CamVidDataset(csv_file=val_file, flip_rate=0, rand_crop=False)\n",
        "val_loader = DataLoader(val_data, batch_size=1, num_workers=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKvpt9GTefaL",
        "colab_type": "text"
      },
      "source": [
        "## Network Model\n",
        "### VGG16 Feature Extractor (pretrained)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUfJoO8e-rrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vgg16(nn.Module):\n",
        "    def __init__(self, pretrained = True):\n",
        "        super(Vgg16, self).__init__()\n",
        "        self.vggnet = models.vgg16(pretrained)\n",
        "        del(self.vggnet.classifier) # Remove fully connected layer to save memory.\n",
        "        features = list(self.vggnet.features)\n",
        "        self.layers = nn.ModuleList(features).eval() \n",
        "        \n",
        "    def forward(self, x):\n",
        "        results = []\n",
        "        for ii,model in enumerate(self.layers):\n",
        "            x = model(x)\n",
        "            if ii in [3,8,15,22,29]:\n",
        "                results.append(x) #(64,256,256),(128,128,128),(256,64,64),(512,32,32),(512,16,16)\n",
        "        return results\n",
        "\n",
        "vgg_model = Vgg16()\n",
        "vgg_model = vgg_model.cuda()\n",
        "print(vgg_model.layers)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10fl6vwtE2Dz",
        "colab_type": "text"
      },
      "source": [
        "### Encoder-Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVmJVvvj1W9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DeConv2d(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size, stride, padding, dilation):\n",
        "        super().__init__()\n",
        "        #####################################\n",
        "        #TODO\n",
        "        #####################################\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #####################################\n",
        "        #TODO\n",
        "        #####################################\n",
        "        return output\n",
        "\n",
        "class EncoderDecoder(nn.Module):\n",
        "    def __init__(self, pretrained_net, n_class):\n",
        "        super().__init__()\n",
        "        self.n_class = n_class\n",
        "        self.pretrained_net = pretrained_net\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.deconv1 = DeConv2d(512, 512, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "        self.bn1 = nn.BatchNorm2d(512)\n",
        "        \n",
        "        self.deconv2 = DeConv2d(512, 256, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "        self.bn2 = nn.BatchNorm2d(256)\n",
        "        \n",
        "        self.deconv3 = DeConv2d(256, 128, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        \n",
        "        self.deconv4 = DeConv2d(128, 64, kernel_size=3, stride=1, padding=1, dilation=1)\n",
        "        self.bn4 = nn.BatchNorm2d(64)\n",
        "        \n",
        "        self.classifier = nn.Conv2d(64, n_class, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #####################################\n",
        "        #TODO\n",
        "        #####################################\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-J1MWQfewD9",
        "colab_type": "text"
      },
      "source": [
        "### Fully Convolution Network (FCN)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MjZ4-8X1EzFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FCN(nn.Module):\n",
        "    def __init__(self, pretrained_net, n_class):\n",
        "        super().__init__()\n",
        "        #####################################\n",
        "        #TODO\n",
        "        #####################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        #####################################\n",
        "        #TODO\n",
        "        #####################################\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5xIH1GffFXP",
        "colab_type": "text"
      },
      "source": [
        "### U-Net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbAKDtkwJQH7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, pretrained_net, n_class):\n",
        "        super().__init__()\n",
        "        #####################################\n",
        "        #TODO\n",
        "        #####################################\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #####################################\n",
        "        #TODO\n",
        "        #####################################\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwISqBr0yMTQ",
        "colab_type": "text"
      },
      "source": [
        "### PSPNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttK5Y9AuyJ7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PSPNet(nn.Module):\n",
        "    def __init__(self, pretrained_net, n_class):\n",
        "        super().__init__()\n",
        "        #####################################\n",
        "        #TODO\n",
        "        #####################################\n",
        "\n",
        "    def forward(self, x):\n",
        "        #####################################\n",
        "        #TODO\n",
        "        #####################################\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjcje1cPfIYs",
        "colab_type": "text"
      },
      "source": [
        "Construct models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oj3Ng3mi1deU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seg_model = EncoderDecoder(pretrained_net=vgg_model, n_class=num_class)\n",
        "#seg_model = FCN(pretrained_net=vgg_model, n_class=num_class)\n",
        "#seg_model = UNet(pretrained_net=vgg_model, n_class=num_class)\n",
        "#seg_model = PSPNet(pretrained_net=vgg_model, n_class=num_class)\n",
        "\n",
        "seg_model = seg_model.cuda()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(seg_model.parameters(), lr=lr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9beJVr7yfNKT",
        "colab_type": "text"
      },
      "source": [
        "## Training and Validation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQtApd8vLCa8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    for epoch in range(epochs):\n",
        "        ts = time.time()\n",
        "        for iter, batch in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "            inputs = torch.FloatTensor(batch['X'])\n",
        "            labels = torch.FloatTensor(batch['Y'])\n",
        "            if use_gpu:\n",
        "              inputs = inputs.cuda()\n",
        "              labels = labels.cuda()\n",
        "\n",
        "            outputs = seg_model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if iter % 10 == 0:\n",
        "                print(\"epoch:{:2}, iter:{:2}, loss: {:.4f}\".format(epoch, iter, loss.data.item()))\n",
        "        \n",
        "        print(\"Finish epoch:{:2}, time elapsed: {:.4f}\".format(epoch, time.time() - ts))\n",
        "        validate()\n",
        "        print(\"========================================\")\n",
        "        \n",
        "    highest_pixel_acc = max(pixel_acc_list)\n",
        "    highest_mIOU = max(mIOU_list)        \n",
        "    \n",
        "    highest_pixel_acc_epoch = pixel_acc_list.index(highest_pixel_acc)\n",
        "    highest_mIOU_epoch = mIOU_list.index(highest_mIOU)\n",
        "    \n",
        "    print(\"The highest mIOU is {} and is achieved at epoch-{}\".format(highest_mIOU, highest_mIOU_epoch))\n",
        "    print(\"The highest pixel accuracy  is {} and is achieved at epoch-{}\".format(highest_pixel_acc, highest_pixel_acc_epoch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtvW9ZqJnmLE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate():\n",
        "    seg_model.eval()\n",
        "    total_ious = []\n",
        "    pixel_accs = []\n",
        "                    \n",
        "    for iter, batch in enumerate(val_loader): ## batch is 1 in this case\n",
        "        inputs = torch.FloatTensor(batch['X'])\n",
        "        if use_gpu:\n",
        "          inputs = inputs.cuda()      \n",
        "\n",
        "        output = seg_model(inputs)                                \n",
        "        \n",
        "        # only save the 1st image for comparison\n",
        "        if iter == 0:\n",
        "            # generate images\n",
        "            images = output.data.max(1)[1].cpu().numpy()[:,:,:]\n",
        "            image = images[0,:,:]        \n",
        "            save_result(batch['X'], image)\n",
        "                            \n",
        "        output = output.data.cpu().numpy()\n",
        "\n",
        "        N, _, h, w = output.shape                \n",
        "        pred = output.transpose(0, 2, 3, 1).reshape(-1, num_class).argmax(axis=1).reshape(N, h, w)        \n",
        "        target = batch['l'].cpu().numpy().reshape(N, h, w)\n",
        "\n",
        "        for p, t in zip(pred, target):\n",
        "            total_ious.append(iou(p, t))\n",
        "            pixel_accs.append(pixel_acc(p, t))\n",
        "\n",
        "    # Calculate average IoU\n",
        "    total_ious = np.array(total_ious).T  # n_class * val_len\n",
        "    ious = np.nanmean(total_ious, axis=1)\n",
        "    pixel_accs = np.array(pixel_accs).mean()\n",
        "    print(\"pix_acc: {:.4f}, meanIoU: {:.4f}\".format(pixel_accs, np.nanmean(ious)))\n",
        "    \n",
        "    global pixel_acc_list\n",
        "    global mIOU_list\n",
        "    \n",
        "    pixel_acc_list.append(pixel_accs)\n",
        "    mIOU_list.append(np.nanmean(ious))\n",
        "\n",
        "# Calculates class intersections over unions\n",
        "def iou(pred, target):\n",
        "    ious = []\n",
        "    for cls in range(num_class):\n",
        "        pred_inds = pred == cls\n",
        "        target_inds = target == cls\n",
        "        intersection = pred_inds[target_inds].sum()\n",
        "        union = pred_inds.sum() + target_inds.sum() - intersection\n",
        "        if union == 0:\n",
        "            ious.append(float('nan'))  # if there is no ground truth, do not include in evaluation\n",
        "        else:\n",
        "            ious.append(float(intersection) / max(union, 1))\n",
        "    return ious\n",
        "\n",
        "def pixel_acc(pred, target):\n",
        "    correct = (pred == target).sum()\n",
        "    total   = (target == target).sum()\n",
        "    return correct / total     \n",
        "\n",
        "def save_result(input_np, output_np):\n",
        "    global global_index\n",
        "    \n",
        "    original_im_RGB = np.zeros((256,256,3))    \n",
        "    original_im_RGB[:,:,0] = input_np[0,0,:,:]    \n",
        "    original_im_RGB[:,:,1] = input_np[0,1,:,:]\n",
        "    original_im_RGB[:,:,2] = input_np[0,2,:,:]\n",
        "        \n",
        "    original_im_RGB[:,:,0] = original_im_RGB[:,:,0] \n",
        "    original_im_RGB[:,:,1] = original_im_RGB[:,:,1] \n",
        "    original_im_RGB[:,:,2] = original_im_RGB[:,:,2] \n",
        "        \n",
        "    original_im_RGB[:,:,0] = original_im_RGB[:,:,0]*255.0\n",
        "    original_im_RGB[:,:,1] = original_im_RGB[:,:,1]*255.0\n",
        "    original_im_RGB[:,:,2] = original_im_RGB[:,:,2]*255.0\n",
        "    \n",
        "    im_seg_RGB = np.zeros((256,256,3))\n",
        "\n",
        "    # the following version is designed for 11-class version and could still work if the number of classes is fewer.\n",
        "    for i in range(256):\n",
        "        for j in range(256):\n",
        "            if output_np[i,j] == 0:\n",
        "                im_seg_RGB[i,j,:] = [128, 128, 128]\n",
        "            elif output_np[i,j] == 1:  \n",
        "                im_seg_RGB[i,j,:] = [128, 0, 0]\n",
        "            elif output_np[i,j] == 2:  \n",
        "                im_seg_RGB[i,j,:] = [192, 192, 128]    \n",
        "            elif output_np[i,j] == 3:  \n",
        "                im_seg_RGB[i,j,:] = [128, 64, 128]    \n",
        "            elif output_np[i,j] == 4:  \n",
        "                im_seg_RGB[i,j,:] = [0, 0, 192]    \n",
        "            elif output_np[i,j] == 5:  \n",
        "                im_seg_RGB[i,j,:] = [128, 128, 0]    \n",
        "            elif output_np[i,j] == 6:  \n",
        "                im_seg_RGB[i,j,:] = [192, 128, 128]    \n",
        "            elif output_np[i,j] == 7:  \n",
        "                im_seg_RGB[i,j,:] = [64, 64, 128]    \n",
        "            elif output_np[i,j] == 8:  \n",
        "                im_seg_RGB[i,j,:] = [64, 0, 128]    \n",
        "            elif output_np[i,j] == 9:  \n",
        "                im_seg_RGB[i,j,:] = [64, 64, 0]    \n",
        "            elif output_np[i,j] == 10:  \n",
        "                im_seg_RGB[i,j,:] = [0, 128, 192]    \n",
        "                    \n",
        "    # horizontally stack original image and its corresponding segmentation results     \n",
        "    hstack_image = np.hstack((original_im_RGB, im_seg_RGB))             \n",
        "    new_im = Image.fromarray(np.uint8(hstack_image))\n",
        "    file_name = val_dir + str(global_index).zfill(3) + '.jpg'\n",
        "    global_index = global_index + 1\n",
        "    new_im.save(file_name)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q80RO6M5JmE5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# perform training and validation\n",
        "train()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}