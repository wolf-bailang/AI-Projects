{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle, json, re, time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "#from tqdm import tqdm\n",
    "from tqdm import trange\n",
    "\n",
    "from gensim.parsing import remove_stopwords\n",
    "\n",
    "import os\n",
    "\n",
    "CWD = os.getcwd()\n",
    "if 'task1' not in CWD:\n",
    "    CWD = os.path.join(CWD, 'task1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter logging and tuning\n",
    "For tuning best models, you may need to save the used hyperparameters.<br />\n",
    "The two cells below make the logging convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function for hyperparameters logging\n",
    "import configparser\n",
    "\n",
    "def write_config(filename, with_time=False):\n",
    "    config = configparser.ConfigParser()\n",
    "    config['DEFAULT'] = {'embedding_dim': embedding_dim,\n",
    "                         'hidden_dim': hidden_dim,\n",
    "                         'learning_rate': learning_rate,\n",
    "                         'max_epoch': max_epoch,\n",
    "                         'batch_size': batch_size}\n",
    "    \n",
    "    if with_time == False:\n",
    "        with open(\"{}.ini\".format(filename), 'w') as configfile:\n",
    "            config.write(configfile)\n",
    "        return 'config'            \n",
    "    else:\n",
    "        timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = filename + '_' + timestr\n",
    "        with open(\"{}.ini\".format(filename), 'w') as configfile:\n",
    "            config.write(configfile)\n",
    "        return ( 'config' + timestr )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyperparameters tuning\n",
    "### Run this cell for renewing the hyperparameters\n",
    "\n",
    "embedding_dim = 100\n",
    "hidden_dim = 512\n",
    "learning_rate = 1e-4\n",
    "max_epoch = 10\n",
    "batch_size = 16\n",
    "\n",
    "# write the hyperparameters into config.ini\n",
    "#write_config(os.path.join(CWD,\"config\"))\n",
    "\n",
    "# if you are lazy to rename the config file then uncomment the below line\n",
    "config_fname = write_config(os.path.join(CWD,\"config\"), True)\n",
    "# config_fname will be used when logging training scalar to tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv( os.path.join(CWD,'data/task1_trainset.csv'), dtype=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove (current) redundant columns.\n",
    "\n",
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)\n",
    "dataset['Abstract'] = dataset['Abstract'].str.lower()\n",
    "#dataset['Task 1'] = dataset['Task 1'].str.lower()\n",
    "\n",
    "for i in range(len(dataset['Abstract'])):\n",
    "    dataset['Abstract'][i] = remove_stopwords(dataset['Abstract'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set test_size=0.1 for validation split\n",
    "trainset, validset = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "trainset.to_csv(os.path.join(CWD,'data/trainset.csv'),index=False)\n",
    "validset.to_csv(os.path.join(CWD,'data/validset.csv'),index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove (current) redundant columns of the test set.\n",
    "\n",
    "dataset = pd.read_csv(os.path.join(CWD,'data/task1_public_testset.csv'), dtype=str)\n",
    "dataset.drop('Title',axis=1,inplace=True)\n",
    "dataset.drop('Categories',axis=1,inplace=True)\n",
    "dataset.drop('Created Date',axis=1, inplace=True)\n",
    "dataset.drop('Authors',axis=1,inplace=True)\n",
    "dataset['Abstract'] = dataset['Abstract'].str.lower()\n",
    "\n",
    "for i in range(len(dataset['Abstract'])):\n",
    "    dataset['Abstract'][i] = remove_stopwords(dataset['Abstract'][i])\n",
    "\n",
    "dataset.to_csv(os.path.join(CWD,'data/testset.csv'),index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect words and create the vocabulary set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def collect_words(data_path, n_workers=4):\n",
    "    df = pd.read_csv(data_path, dtype=str)\n",
    "        \n",
    "    # create a list for storing sentences\n",
    "    sent_list = []\n",
    "    for i in df.iterrows():\n",
    "        # remove $$$ and append to sent_list\n",
    "        sent_list += i[1]['Abstract'].split('$$$')\n",
    "\n",
    "    chunks = [\n",
    "        ' '.join(sent_list[i:i + len(sent_list) // n_workers])\n",
    "        for i in range(0, len(sent_list), len(sent_list) // n_workers)\n",
    "    ]\n",
    "    with Pool(n_workers) as pool:\n",
    "        # word_tokenize for word-word separation\n",
    "        chunks = pool.map_async(word_tokenize, chunks)\n",
    "        words = set(sum(chunks.get(), []))\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set()\n",
    "words |= collect_words(os.path.join(CWD,'data/trainset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "UNK_TOKEN = 1\n",
    "word_dict = {'<pad>':PAD_TOKEN,'<unk>':UNK_TOKEN}\n",
    "for word in words:\n",
    "    word_dict[word]=len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(CWD,'dicitonary.pkl'),'wb') as f:\n",
    "    pickle.dump(word_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Glove pretrained word embedding from web.\n",
    "\n",
    "Link: http://nlp.stanford.edu/data/glove.6B.zip <br />\n",
    "It takes about 5 minutes for the download.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "if not os.path.exists('glove'):\n",
    "    os.mkdir('glove')\n",
    "    r = requests.get('http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    z.extractall(path='glove')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the GloVe word-embeddings file\n",
    "\n",
    "Parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation (as number vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parsing the GloVe word-embeddings file\n",
    "# Parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation (as number vectors)\n",
    "\n",
    "wordvector_path = 'glove/glove.6B.100d.txt'\n",
    "embeddings_index = {}\n",
    "f = open(wordvector_path)\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Preparing the GloVe word-embeddings matrix\n",
    "\n",
    "max_words = len(word_dict)\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in word_dict.items():\n",
    "    #if i < max_words:\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = torch.FloatTensor(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Formatting\n",
    "建立完字典後，我們要將 data 切成數個 batch，並且將輸入的句子轉成數字，將答案轉成 onehot vector。\n",
    "- `label_to_onehot(labels)`:  \n",
    "    將 datasert 中的 label string 轉成 onehot encoding vector。  \n",
    "- `sentence_to_indices(sentence, word_dict)`:  \n",
    "    將輸入的句子中每個 word 轉成字典中對應的 index  \n",
    "    ex : 'i love ncku' -> $[1,2,3]$\n",
    "- `get_dataset(data_path, word_dict, n_workers=4)`:  \n",
    "    將 dataset 讀入\n",
    "- `preprocess_samples(dataset, word_dict)`:  \n",
    "    傳入所有 input sentences 並進行 data preprocessing  \n",
    "- `preprocess_sample(data, word_dict)`:  \n",
    "    主要透過這個 function 移除存在於 'Abstract' 中的 `$` 符號  \n",
    "    並將 'Label' 轉成 onehot encoding vector。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_to_onehot(labels):\n",
    "    \"\"\" Convert label to onehot .\n",
    "        Args:\n",
    "            labels (string): sentence's labels.\n",
    "        Return:\n",
    "            outputs (onehot list): sentence's onehot label.\n",
    "    \"\"\"\n",
    "    label_dict = {'BACKGROUND': 0, 'OBJECTIVES':1, 'METHODS':2, 'RESULTS':3, 'CONCLUSIONS':4, 'OTHERS':5}\n",
    "    onehot = [0,0,0,0,0,0]\n",
    "    for l in labels.split('/'):\n",
    "        onehot[label_dict[l]] = 1\n",
    "    return onehot\n",
    "        \n",
    "def sentence_to_indices(sentence, word_dict):\n",
    "    \"\"\" Convert sentence to its word indices.\n",
    "    Args:\n",
    "        sentence (str): One string.\n",
    "    Return:\n",
    "        indices (list of int): List of word indices.\n",
    "    \"\"\"\n",
    "    return [word_dict.get(word,UNK_TOKEN) for word in word_tokenize(sentence)]\n",
    "    \n",
    "def get_dataset(data_path, word_dict, n_workers=4):\n",
    "    \"\"\" Load data and return dataset for training and validating.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data.\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(data_path, dtype=str)\n",
    "\n",
    "    results = [None] * n_workers\n",
    "    with Pool(processes=n_workers) as pool:\n",
    "        for i in range(n_workers):\n",
    "            batch_start = (len(dataset) // n_workers) * i\n",
    "            if i == n_workers - 1:\n",
    "                batch_end = len(dataset)\n",
    "            else:\n",
    "                batch_end = (len(dataset) // n_workers) * (i + 1)\n",
    "            \n",
    "            batch = dataset[batch_start: batch_end]\n",
    "            results[i] = pool.apply_async(preprocess_samples, args=(batch,word_dict))\n",
    "\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    processed = []\n",
    "    for result in results:\n",
    "        processed += result.get()\n",
    "    return processed\n",
    "\n",
    "def preprocess_samples(dataset, word_dict):\n",
    "    \"\"\" Worker function.\n",
    "\n",
    "    Args:\n",
    "        dataset (list of dict)\n",
    "    Returns:\n",
    "        list of processed dict.\n",
    "    \"\"\"\n",
    "    processed = []\n",
    "    for sample in tqdm(dataset.iterrows(), total=len(dataset)):\n",
    "        processed.append(preprocess_sample(sample[1], word_dict))\n",
    "\n",
    "    return processed\n",
    "\n",
    "def preprocess_sample(data, word_dict):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        data (dict)\n",
    "    Returns:\n",
    "        dict\n",
    "    \"\"\"\n",
    "    ## clean abstracts by removing $$$\n",
    "    processed = {}\n",
    "    processed['Abstract'] = [sentence_to_indices(sent, word_dict) for sent in data['Abstract'].split('$$$')]\n",
    "    \n",
    "    ## convert the labels into one-hot encoding\n",
    "    if 'Task 1' in data:\n",
    "        processed['Label'] = [label_to_onehot(label) for label in data['Task 1'].split(' ')]\n",
    "        \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('[INFO] Start processing trainset...')\n",
    "train = get_dataset(os.path.join(CWD,'data/trainset.csv'), word_dict, n_workers=4)\n",
    "print('[INFO] Start processing validset...')\n",
    "valid = get_dataset(os.path.join(CWD,'data/validset.csv'), word_dict, n_workers=4)\n",
    "print('[INFO] Start processing testset...')\n",
    "test = get_dataset(os.path.join(CWD,'data/testset.csv'), word_dict, n_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset class for the abstract dataset\n",
    "`torch.utils.data.Dataset` is an abstract class representing a dataset.<br />Your custom dataset should inherit Dataset and override the following methods:\n",
    "\n",
    "- `__len__` so that len(dataset) returns the size of the dataset.\n",
    "- `__getitem__` to support the indexing such that dataset[i] can be used to get i\n",
    "th sample\n",
    "- `collate_fn` Users may use customized collate_fn to achieve custom batching\n",
    "    - Here we pad sequences of various lengths (make same length of every single sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractDataset(Dataset):\n",
    "    def __init__(self, data, pad_idx, max_len = 64):\n",
    "        self.data = data\n",
    "        self.pad_idx = pad_idx\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "        \n",
    "    def collate_fn(self, datas):\n",
    "        # get max length in this batch\n",
    "        max_sent = max([len(data['Abstract']) for data in datas])\n",
    "        max_len = max([min(len(sentence), self.max_len) for data in datas for sentence in data['Abstract']])\n",
    "        batch_abstract = []\n",
    "        batch_label = []\n",
    "        sent_len = []\n",
    "        for data in datas:\n",
    "            # padding abstract to make them in same length\n",
    "            pad_abstract = []\n",
    "            for sentence in data['Abstract']:\n",
    "                if len(sentence) > max_len:\n",
    "                    pad_abstract.append(sentence[:max_len])\n",
    "                else:\n",
    "                    pad_abstract.append(sentence+[self.pad_idx]*(max_len-len(sentence)))\n",
    "            sent_len.append(len(pad_abstract))\n",
    "            pad_abstract.extend([[self.pad_idx]*max_len]*(max_sent-len(pad_abstract)))\n",
    "            batch_abstract.append(pad_abstract)\n",
    "            # gather labels\n",
    "            if 'Label' in data:\n",
    "                pad_label = data['Label']\n",
    "                pad_label.extend([[0]*6]*(max_sent-len(pad_label)))\n",
    "                \n",
    "                batch_label.append(pad_label)\n",
    "        return torch.LongTensor(batch_abstract), torch.FloatTensor(batch_label), sent_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = AbstractDataset(train, PAD_TOKEN, max_len = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData = AbstractDataset(train, PAD_TOKEN, max_len = 64)\n",
    "validData = AbstractDataset(valid, PAD_TOKEN, max_len = 64)\n",
    "testData = AbstractDataset(test, PAD_TOKEN, max_len = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocabulary_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding_size = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.embedding = nn.Embedding(vocabulary_size, self.embedding_size)\n",
    "        self.embedding.weight = torch.nn.Parameter(embedding_matrix)\n",
    "        self.sent_rnn = nn.GRU(self.embedding_size,\n",
    "                                self.hidden_dim,\n",
    "                                bidirectional=True,\n",
    "                                batch_first=True)\n",
    "        \n",
    "        self.l1 = nn.Linear(self.hidden_dim*2, self.hidden_dim)\n",
    "        torch.nn.init.xavier_normal_(self.l1.weight)\n",
    "        #self.layernorm1 = nn.LayerNorm(self.hidden_dim*2)\n",
    "        #self.layernorm2 = nn.LayerNorm(self.hidden_dim)\n",
    "        self.l2 = nn.Linear(self.hidden_dim, 6)\n",
    "\n",
    "    # b: batch_size\n",
    "    # s: number of sentences\n",
    "    # w: number of words\n",
    "    # e: embedding_dim\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        b,s,w,e = x.shape\n",
    "        x = x.view(b,s*w,e)\n",
    "        x, __ = self.sent_rnn(x)\n",
    "        x = x.view(b,s,w,-1)\n",
    "        x = torch.max(x,dim=2)[0]\n",
    "        #final_ht = x[-1]\n",
    "        \n",
    "        #x = self.layernorm1(x)\n",
    "        \n",
    "        x = torch.relu(self.l1(x))\n",
    "        \n",
    "        #x = self.layernorm2(x)\n",
    "        \n",
    "        x = torch.sigmoid(self.l2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper functions for scoring\n",
    "\n",
    "class F1():\n",
    "    def __init__(self):\n",
    "        self.threshold = 0.5\n",
    "        self.n_precision = 0\n",
    "        self.n_recall = 0\n",
    "        self.n_corrects = 0\n",
    "        self.name = 'F1'\n",
    "\n",
    "    def reset(self):\n",
    "        self.n_precision = 0\n",
    "        self.n_recall = 0\n",
    "        self.n_corrects = 0\n",
    "\n",
    "    def update(self, predicts, groundTruth):\n",
    "        predicts = predicts > self.threshold\n",
    "        self.n_precision += torch.sum(predicts).data.item()\n",
    "        self.n_recall += torch.sum(groundTruth).data.item()\n",
    "        self.n_corrects += torch.sum(groundTruth.type(torch.bool) * predicts).data.item()\n",
    "\n",
    "    def get_score(self):\n",
    "        recall = self.n_corrects / self.n_recall\n",
    "        precision = self.n_corrects / (self.n_precision + 1e-20)\n",
    "        return 2 * (recall * precision) / (recall + precision + 1e-20)\n",
    "\n",
    "    def print_score(self):\n",
    "        score = self.get_score()\n",
    "        return '{:.5f}'.format(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_epoch(epoch, mode):\n",
    "    model.train(True)\n",
    "    if mode==\"train\":\n",
    "        description = 'Train'\n",
    "        dataset = trainData\n",
    "        shuffle = True\n",
    "    else:\n",
    "        description = 'Valid'\n",
    "        dataset = validData\n",
    "        shuffle = False\n",
    "    dataloader = DataLoader(dataset=dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=shuffle,\n",
    "                            collate_fn=dataset.collate_fn,\n",
    "                            num_workers=8)\n",
    "\n",
    "    trange = tqdm(enumerate(dataloader), total=len(dataloader), desc=description)\n",
    "    loss = 0\n",
    "    f1_score = F1()\n",
    "    for i, (x, y, sent_len) in trange:\n",
    "        o_labels, batch_loss = _run_iter(x,y)\n",
    "        if mode==\"train\":\n",
    "            opt.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        loss += batch_loss.item()\n",
    "        f1_score.update(o_labels.cpu(), y)\n",
    "\n",
    "        trange.set_postfix(\n",
    "            loss=loss / (i + 1), f1=f1_score.print_score())\n",
    "    \n",
    "    if mode==\"train\":\n",
    "        history['train'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "        writer.add_scalar('Loss/train', loss/ len(trange), epoch)\n",
    "        writer.add_scalar('F1_score/train', f1_score.get_score(), epoch)\n",
    "    else:\n",
    "        history['valid'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n",
    "        writer.add_scalar('Loss/valid', loss/ len(trange), epoch)\n",
    "        writer.add_scalar('F1_score/valid', f1_score.get_score(), epoch)\n",
    "    trange.close()\n",
    "    \n",
    "\n",
    "def _run_iter(x,y):\n",
    "    abstract = x.to(device)\n",
    "    labels = y.to(device)\n",
    "    o_labels = model(abstract)\n",
    "    l_loss = criteria(o_labels, labels)\n",
    "    return o_labels, l_loss\n",
    "\n",
    "def save(epoch):\n",
    "    if not os.path.exists(os.path.join(CWD,'model')):\n",
    "        os.makedirs(os.path.join(CWD,'model'))\n",
    "    torch.save(model.state_dict(), os.path.join( CWD,'model/model.pkl.'+str(epoch) ))\n",
    "    with open( os.path.join( CWD,'model/history.json'), 'w') as f:\n",
    "        json.dump(history, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(len(word_dict))\n",
    "\n",
    "opt = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "criteria = torch.nn.BCELoss()\n",
    "model.to(device)\n",
    "history = {'train':[],'valid':[]}\n",
    "\n",
    "## Tensorboard\n",
    "## save path: test_experiment/\n",
    "tf_path = os.path.join(CWD, 'test_experiment')\n",
    "if not os.path.exists(tf_path):\n",
    "    os.mkdir(tf_path)\n",
    "writer = SummaryWriter(os.path.join(tf_path,config_fname))\n",
    "\n",
    "for epoch in range(max_epoch):\n",
    "    print('Epoch: {}'.format(epoch))\n",
    "    _run_epoch(epoch, 'train')\n",
    "    _run_epoch(epoch, 'valid')\n",
    "    save(epoch)\n",
    "\n",
    "# Plot the training results \n",
    "with open(os.path.join(CWD,'model/history.json'), 'r') as f:\n",
    "    history = json.loads(f.read())\n",
    "    \n",
    "train_loss = [l['loss'] for l in history['train']]\n",
    "valid_loss = [l['loss'] for l in history['valid']]\n",
    "train_f1 = [l['f1'] for l in history['train']]\n",
    "valid_f1 = [l['f1'] for l in history['valid']]\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title('Loss')\n",
    "plt.plot(train_loss, label='train')\n",
    "plt.plot(valid_loss, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.title('F1 Score')\n",
    "plt.plot(train_f1, label='train')\n",
    "plt.plot(valid_f1, label='valid')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print('Best F1 score ', max([[l['f1'], idx] for idx, l in enumerate(history['valid'])]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the Prediction cell.\n",
    "\n",
    "# fill the epoch of the lowest val_loss to best_model\n",
    "best_model = 9\n",
    "model.load_state_dict(state_dict=torch.load(os.path.join(CWD,'model/model.pkl.{}'.format(best_model))))\n",
    "model.train(False)\n",
    "# double ckeck the best_model_score\n",
    "_run_epoch(1, 'valid')\n",
    "\n",
    "# start testing\n",
    "dataloader = DataLoader(dataset=testData,\n",
    "                            batch_size=64,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=testData.collate_fn,\n",
    "                            num_workers=8)\n",
    "trange = tqdm(enumerate(dataloader), total=len(dataloader), desc='Predict')\n",
    "prediction = []\n",
    "for i, (x, y, sent_len) in trange:\n",
    "    o_labels = model(x.to(device))\n",
    "    o_labels = o_labels>0.5\n",
    "    for idx, o_label in enumerate(o_labels):\n",
    "        prediction.append(o_label[:sent_len[idx]].to('cpu'))\n",
    "prediction = torch.cat(prediction).detach().numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function for creating a csv file following the submission format\n",
    "\n",
    "def SubmitGenerator(prediction, sampleFile, public=True, filename='prediction.csv'):\n",
    "    sample = pd.read_csv(sampleFile)\n",
    "    submit = {}\n",
    "    submit['order_id'] = list(sample.order_id.values)\n",
    "    redundant = len(sample) - prediction.shape[0]\n",
    "    if public:\n",
    "        submit['BACKGROUND'] = list(prediction[:,0]) + [0]*redundant\n",
    "        submit['OBJECTIVES'] = list(prediction[:,1]) + [0]*redundant\n",
    "        submit['METHODS'] = list(prediction[:,2]) + [0]*redundant\n",
    "        submit['RESULTS'] = list(prediction[:,3]) + [0]*redundant\n",
    "        submit['CONCLUSIONS'] = list(prediction[:,4]) + [0]*redundant\n",
    "        submit['OTHERS'] = list(prediction[:,5]) + [0]*redundant\n",
    "    else:\n",
    "        submit['BACKGROUND'] = [0]*redundant + list(prediction[:,0])\n",
    "        submit['OBJECTIVES'] = [0]*redundant + list(prediction[:,1])\n",
    "        submit['METHODS'] = [0]*redundant + list(prediction[:,2])\n",
    "        submit['RESULTS'] = [0]*redundant + list(prediction[:,3])\n",
    "        submit['CONCLUSIONS'] = [0]*redundant + list(prediction[:,4])\n",
    "        submit['OTHERS'] = [0]*redundant + list(prediction[:,5])\n",
    "    df = pd.DataFrame.from_dict(submit) \n",
    "    df.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Output csv for submission\n",
    "\n",
    "SubmitGenerator(prediction,\n",
    "                os.path.join(CWD,'data/task1_sample_submission.csv'), \n",
    "                True, \n",
    "                os.path.join(CWD,'submission_1106.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir=task1/test_experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
