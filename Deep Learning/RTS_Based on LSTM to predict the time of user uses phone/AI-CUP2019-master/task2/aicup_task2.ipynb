{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.5"},"colab":{"name":"aicup_task2.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"code","metadata":{"id":"ncUe6GKNF1LB","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":127},"outputId":"66a75136-a649-4515-d44e-3f35b2a2c239","executionInfo":{"status":"ok","timestamp":1576153955966,"user_tz":-480,"elapsed":18155,"user":{"displayName":"Junbin Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCvGWXPyI9C7kW0OBYIcomRgTWv4-asJoXajP0E=s64","userId":"05440645963175947010"}}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZF0oIOqVF1LH","colab_type":"text"},"source":["### Hyperparameter logging and tuning\n","For tuning best models, you may need to save the used hyperparameters.<br />\n","The two cells below make the logging convenient."]},{"cell_type":"code","metadata":{"id":"YHNg_E2iF1LI","colab_type":"code","colab":{}},"source":["### Helper function for hyperparameters logging\n","import configparser\n","\n","def write_config(filename, with_time=False):\n","    config = configparser.ConfigParser()\n","    config['DEFAULT'] = {'embedding_dim': embedding_dim,\n","                         'hidden_dim': hidden_dim,\n","                         'learning_rate': learning_rate,\n","                         'max_epoch': max_epoch,\n","                         'batch_size': batch_size}\n","    \n","    if with_time == False:\n","        with open(\"{}.ini\".format(filename), 'w') as configfile:\n","            config.write(configfile)  \n","        return 'config'          \n","    else:\n","        timestr = time.strftime(\"%Y%m%d_%H%M%S\")\n","        filename = filename + '_' + timestr\n","        with open(\"{}.ini\".format(filename), 'w') as configfile:\n","            config.write(configfile)\n","        return ( 'config' + timestr )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mRWb4f9eF1LL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":354},"outputId":"fea37d6a-0b44-4c47-eeef-77a1fda3e168","executionInfo":{"status":"error","timestamp":1576155192467,"user_tz":-480,"elapsed":752,"user":{"displayName":"Junbin Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCvGWXPyI9C7kW0OBYIcomRgTWv4-asJoXajP0E=s64","userId":"05440645963175947010"}}},"source":["### Hyperparameters tuning\n","### Run this cell for renewing the hyperparameters\n","\n","embedding_dim = 300\n","hidden_dim = 512\n","learning_rate = 2e-4\n","max_epoch = 6\n","batch_size = 8\n","\n","\n","# write the hyperparameters into config.ini\n","#write_config(os.path.join(CWD,\"config\"))\n","\n","# if you are lazy to rename the config file then uncomment the below line\n","config_fname = write_config(os.path.join(CWD,\"config\"), True)\n","# config_fname will be used when logging training scalar to tensorboard"],"execution_count":6,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-910bceaf4a1d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# if you are lazy to rename the config file then uncomment the below line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mconfig_fname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrite_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCWD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"config\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m# config_fname will be used when logging training scalar to tensorboard\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-5429d1a7a4be>\u001b[0m in \u001b[0;36mwrite_config\u001b[0;34m(filename, with_time)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mtimestr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%d_%H%M%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimestr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}.ini\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconfigfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m \u001b[0;34m'config'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtimestr\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/task2/config_20191212_125311.ini'"]}]},{"cell_type":"markdown","metadata":{"id":"wkcfKUoXF1LN","colab_type":"text"},"source":["### Dataset pre-processing\n","Id: 流水號\n","Title: 論文標題\n","Abstract: 論文摘要內容, 句子間以 $$$ 分隔\n","Authors: 論文作者\n","Categories: 論文類別\n","Created date: 論文上傳日期\n","Task 2: 論文分類類別, 若句子有多個類別,以 空格 分隔"]},{"cell_type":"code","metadata":{"id":"P8_IsATgF1LO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":414},"outputId":"561915ed-c630-4216-98de-da807278948e","executionInfo":{"status":"error","timestamp":1576155198845,"user_tz":-480,"elapsed":920,"user":{"displayName":"Junbin Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCvGWXPyI9C7kW0OBYIcomRgTWv4-asJoXajP0E=s64","userId":"05440645963175947010"}}},"source":["dataset = pd.read_csv(os.path.join(CWD,'data/task2_trainset.csv'), dtype=str)\n","dataset.head()"],"execution_count":7,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-50591222cad5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCWD\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'data/task2_trainset.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/content/task2/data/task2_trainset.csv' does not exist: b'/content/task2/data/task2_trainset.csv'"]}]},{"cell_type":"markdown","metadata":{"id":"mCfOIfDmF1LR","colab_type":"text"},"source":["### 刪除多於資訊 (Remove redundant information)\n","我們在資料集中保留了許多額外資訊供大家使用，但是在這次的教學中我們並沒有用到全部資訊，因此先將多餘的部分先抽走。"]},{"cell_type":"code","metadata":{"id":"UpEu1vxBF1LS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":246},"outputId":"89a91774-3c4d-4154-caad-072dfc976861","executionInfo":{"status":"error","timestamp":1576155216244,"user_tz":-480,"elapsed":815,"user":{"displayName":"Junbin Zhang","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mCvGWXPyI9C7kW0OBYIcomRgTWv4-asJoXajP0E=s64","userId":"05440645963175947010"}}},"source":["### Remove (current) redundant columns.\n","\n","dataset.drop('Title',axis=1,inplace=True)\n","dataset.drop('Categories',axis=1,inplace=True)\n","dataset.drop('Created Date',axis=1, inplace=True)\n","dataset.drop('Authors',axis=1,inplace=True)\n","dataset['Abstract'] = dataset['Abstract'].str.lower()"],"execution_count":8,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-dc15a366e944>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Title'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Categories'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Created Date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Authors'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"wRGqadiAF1LV","colab_type":"text"},"source":["### 資料切割 (Partition)\n","在訓練時，我們需要有個方法去檢驗訓練結果的好壞，因此需要將訓練資料切成training/validataion set。"]},{"cell_type":"code","metadata":{"id":"zFLSFntfF1LW","colab_type":"code","colab":{}},"source":["# set test_size=0.1 for validation split\n","trainset, validset = train_test_split(dataset, test_size=0.1, random_state=42)\n","\n","trainset.to_csv(os.path.join(CWD,'data/trainset.csv'), index=False)\n","validset.to_csv(os.path.join(CWD,'data/validset.csv'), index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mSk4VZ48F1LZ","colab_type":"code","colab":{}},"source":["### Remove (current) redundant columns of the test set.\n","\n","dataset = pd.read_csv(os.path.join(CWD,'data/task2_testset.csv'), dtype=str)\n","dataset.drop('Title',axis=1,inplace=True)\n","dataset.drop('Categories',axis=1,inplace=True)\n","dataset.drop('Created Date',axis=1, inplace=True)\n","dataset.drop('Authors',axis=1,inplace=True)\n","dataset['Abstract'] = dataset['Abstract'].str.lower()\n","\n","dataset.to_csv(os.path.join(CWD,'data/testset.csv'),index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LOReIfzhF1Lb","colab_type":"text"},"source":["### 統計單字 (Count words)\n","在訓練時，不能直接將單字直接餵入model，因為它只看得懂數字，因此我們必須把所有的單字抽取出來，並將它們打上編號，做出一個字典來對它們做轉換。 \n","\n","在這裡，我們需要借助nltk這個library來幫忙做文字切割。當然，你也可以選擇自己寫規則來切割(通常上不建議搞死自己)。\n","另外，我們也使用了multiprocessing來加速處理。"]},{"cell_type":"code","metadata":{"id":"2nmyMhuiF1Lc","colab_type":"code","colab":{}},"source":["from multiprocessing import Pool\n","from nltk.tokenize import word_tokenize\n","\n","def collect_words(data_path, n_workers=4):\n","    df = pd.read_csv(data_path, dtype=str)\n","        \n","    sent_list = []\n","    for i in df.iterrows():\n","        sent_list += i[1]['Abstract'].split('$$$')\n","\n","    chunks = [\n","        ' '.join(sent_list[i:i + len(sent_list) // n_workers])\n","        for i in range(0, len(sent_list), len(sent_list) // n_workers)\n","    ]\n","    with Pool(n_workers) as pool:\n","        chunks = pool.map_async(word_tokenize, chunks)\n","        words = set(sum(chunks.get(), []))\n","\n","    return words"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KSoCmr1fF1Lf","colab_type":"code","colab":{}},"source":["words = set()\n","words |= collect_words(os.path.join(CWD,'data/trainset.csv'))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WZF1evEQF1Li","colab_type":"text"},"source":["pad: for padding\n","unk: for word that didn't in our dicitonary"]},{"cell_type":"code","metadata":{"id":"mnB_eBZUF1Lj","colab_type":"code","colab":{}},"source":["PAD_TOKEN = 0\n","UNK_TOKEN = 1\n","word_dict = {'<pad>':PAD_TOKEN,'<unk>':UNK_TOKEN}\n","for word in words:\n","    word_dict[word]=len(word_dict)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-0AdS69tF1Ll","colab_type":"code","colab":{}},"source":["with open(os.path.join(CWD,'dicitonary.pkl'),'wb') as f:\n","    pickle.dump(word_dict, f)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QhxpwFo4F1Lo","colab_type":"text"},"source":["### Download Glove pretrained word embedding from web.\n","\n","Link: http://nlp.stanford.edu/data/glove.6B.zip <br />\n","It takes about 5 minutes for the download.\n","\n"]},{"cell_type":"code","metadata":{"id":"-SeZXw3AF1Lp","colab_type":"code","colab":{}},"source":["import requests, zipfile, io, os\n","if not os.path.exists('glove'):\n","#if not os.path.exists('glove'):\n","    os.mkdir('glove')\n","    r = requests.get('http://nlp.stanford.edu/data/glove.6B.zip')\n","    z = zipfile.ZipFile(io.BytesIO(r.content))\n","    z.extractall(path='glove')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MhJSsmuxF1Ls","colab_type":"text"},"source":["### Parsing the GloVe word-embeddings file\n","\n","Parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation (as number vectors)"]},{"cell_type":"code","metadata":{"id":"5XhHiTVbF1Ls","colab_type":"code","colab":{}},"source":["### Parsing the GloVe word-embeddings file\n","# Parse the unzipped file (a .txt file) to build an index that maps words (as strings) to their vector representation (as number vectors)\n","\n","wordvector_path = 'glove/glove.6B.300d.txt'\n","embeddings_index = {}\n","f = open(wordvector_path)\n","for line in f:\n","    values = line.split()\n","    word = values[0]\n","    coefs = np.asarray(values[1:], dtype='float32')\n","    embeddings_index[word] = coefs\n","f.close()\n","print('Found %s word vectors.' % len(embeddings_index))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cn_XiIxMF1Lv","colab_type":"code","colab":{}},"source":["### Preparing the GloVe word-embeddings matrix\n","\n","max_words = len(word_dict)\n","embedding_matrix = np.zeros((max_words, embedding_dim))\n","for word, i in word_dict.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"B7syM2YSF1Ly","colab_type":"code","colab":{}},"source":["embedding_matrix = torch.FloatTensor(embedding_matrix)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OhJSY4L8F1L0","colab_type":"text"},"source":["### 資料格式化 (Data formatting)\n","有了字典後，接下來我們要把資料整理成一筆一筆，把input的句子轉成數字，把答案轉成onehot的形式。\n","這裡，我們一樣使用multiprocessing來加入進行。"]},{"cell_type":"code","metadata":{"id":"J5OwAGtKF1L1","colab_type":"code","colab":{}},"source":["def label_to_onehot(labels):\n","    \"\"\" Convert label to onehot .\n","        Args:\n","            labels (string): sentence's labels.\n","        Return:\n","            outputs (onehot list): sentence's onehot label.\n","    \"\"\"\n","    label_dict = {'THEORETICAL': 0, 'ENGINEERING':1, 'EMPIRICAL':2, 'OTHERS':3}\n","    onehot = [0,0,0,0]\n","    for l in labels.split():\n","        onehot[label_dict[l]] = 1\n","    return onehot\n","        \n","def sentence_to_indices(sentence, word_dict):\n","    \"\"\" Convert sentence to its word indices.\n","    Args:\n","        sentence (str): One string.\n","    Return:\n","        indices (list of int): List of word indices.\n","    \"\"\"\n","    #return [word_dict.to_index(word) for word in word_tokenize(sentence)]\n","    return [word_dict.get(word,UNK_TOKEN) for word in word_tokenize(sentence)]\n","    \n","def get_dataset(data_path, word_dict, n_workers=4):\n","    \"\"\" Load data and return dataset for training and validating.\n","\n","    Args:\n","        data_path (str): Path to the data.\n","    \"\"\"\n","    dataset = pd.read_csv(data_path, dtype=str)\n","\n","    results = [None] * n_workers\n","    with Pool(processes=n_workers) as pool:\n","        for i in range(n_workers):\n","            batch_start = (len(dataset) // n_workers) * i\n","            if i == n_workers - 1:\n","                batch_end = len(dataset)\n","            else:\n","                batch_end = (len(dataset) // n_workers) * (i + 1)\n","            \n","            batch = dataset[batch_start: batch_end]\n","            results[i] = pool.apply_async(preprocess_samples, args=(batch,word_dict))\n","\n","        pool.close()\n","        pool.join()\n","\n","    processed = []\n","    for result in results:\n","        processed += result.get()\n","    return processed\n","\n","def preprocess_samples(dataset, word_dict):\n","    \"\"\" Worker function.\n","\n","    Args:\n","        dataset (list of dict)\n","    Returns:\n","        list of processed dict.\n","    \"\"\"\n","    processed = []\n","    for sample in tqdm(dataset.iterrows(), total=len(dataset)):\n","        processed.append(preprocess_sample(sample[1], word_dict))\n","\n","    return processed\n","\n","def preprocess_sample(data, word_dict):\n","    \"\"\"\n","    Args:\n","        data (dict)\n","    Returns:\n","        dict\n","    \"\"\"\n","    processed = {}\n","    processed['Abstract'] = [sentence_to_indices(sent, word_dict) for sent in data['Abstract'].split('$$$')]\n","    if 'Task 2' in data:\n","        processed['Label'] = label_to_onehot(data['Task 2'])\n","        \n","    return processed"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o64bDSssF1L4","colab_type":"code","colab":{}},"source":["print('[INFO] Start processing trainset...')\n","train = get_dataset(os.path.join(CWD,'data/trainset.csv'), word_dict, n_workers=4)\n","print('[INFO] Start processing validset...')\n","valid = get_dataset(os.path.join(CWD,'data/validset.csv'), word_dict, n_workers=4)\n","print('[INFO] Start processing testset...')\n","test = get_dataset(os.path.join(CWD,'data/testset.csv'), word_dict, n_workers=4)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qmqiHVVhF1L7","colab_type":"text"},"source":["### Create a dataset class for the abstract dataset\n","`torch.utils.data.Dataset` is an abstract class representing a dataset.<br />Your custom dataset should inherit Dataset and override the following methods:\n","\n","- `__len__` so that len(dataset) returns the size of the dataset.\n","- `__getitem__` to support the indexing such that dataset[i] can be used to get i\n","th sample\n","- `collate_fn` Users may use customized collate_fn to achieve custom batching\n","    - Here we pad sequences of various lengths (make same length of every abstract)"]},{"cell_type":"markdown","metadata":{"id":"hYMj3tPbF1L8","colab_type":"text"},"source":["### 資料封裝 (Data packing)\n","\n","為了更方便的進行batch training，我們將會借助torch.utils.data.DataLoader。\n","而要將資料放入dataloader，我們需要繼承torch.utils.data.Dataset，撰寫適合這份dataset的class。\n","collate_fn用於batch data的後處理，在dataloder將選出的data放進list後會呼叫collate_fn，而我們會在此把sentence padding到同樣的長度，才能夠放入torch tensor (tensor必須為矩陣)。"]},{"cell_type":"code","metadata":{"id":"wX-32i7iF1L-","colab_type":"code","colab":{}},"source":["class AbstractDataset(Dataset):\n","    def __init__(self, data, pad_idx, max_len = 500):\n","        self.data = data\n","        self.pad_idx = pad_idx\n","        self.max_len = max_len\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, index):\n","        return self.data[index]\n","        \n","    def collate_fn(self, datas):\n","        # get max length in this batch\n","        max_sent = max([len(data['Abstract']) for data in datas])\n","        max_len = max([min(len(sentence), self.max_len) for data in datas for sentence in data['Abstract']])\n","        batch_abstract = []\n","        batch_label = []\n","        sent_len = []\n","        for data in datas:\n","            # padding abstract to make them in same length\n","            pad_abstract = []\n","            for sentence in data['Abstract']:\n","                if len(sentence) > max_len:\n","                    pad_abstract.append(sentence[:max_len])\n","                else:\n","                    pad_abstract.append(sentence+[self.pad_idx]*(max_len-len(sentence)))\n","            sent_len.append(len(pad_abstract))\n","            pad_abstract.extend([[self.pad_idx]*max_len]*(max_sent-len(pad_abstract)))\n","            batch_abstract.append(pad_abstract)\n","            \n","            # gather labels\n","            if 'Label' in data:\n","                batch_label.append(data['Label'])\n","                \n","        return torch.LongTensor(batch_abstract), torch.FloatTensor(batch_label), sent_len"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EoeNiuA5F1MC","colab_type":"code","colab":{}},"source":["trainData = AbstractDataset(train, PAD_TOKEN, max_len = 500)\n","validData = AbstractDataset(valid, PAD_TOKEN, max_len = 500)\n","testData = AbstractDataset(test, PAD_TOKEN, max_len = 500)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iGu6XGA4F1MF","colab_type":"text"},"source":["### Model\n","\n","資料處理完成後，接下來就是最重要的核心部分：Model。\n","此次範例中我們以簡單的一層RNN + 一層Linear layer作為示範。\n","而為了解決每次的句子長度不一的問題(linear layer必須是fixed input size)，因此我們把所有字的hidden_state做平均，讓這一個vector代表這句話。"]},{"cell_type":"code","metadata":{"id":"Q_KX3Wf7F1MG","colab_type":"code","colab":{}},"source":["class Net(nn.Module):\n","    def __init__(self, vocabulary_size):\n","        super(Net, self).__init__()\n","        self.embedding_size = embedding_dim\n","        self.hidden_dim = 512\n","        self.embedding = nn.Embedding(vocabulary_size, self.embedding_size)\n","        self.embedding.weight = torch.nn.Parameter(embedding_matrix)\n","        self.sent_rnn = nn.GRU(self.embedding_size,\n","                                self.hidden_dim,\n","                                bidirectional=True,\n","                                batch_first=True)        \n","        self.l1 = nn.Linear(self.hidden_dim, 4)\n","        torch.nn.init.xavier_normal_(self.l1.weight)\n","\n","    def forward(self, x):\n","        x = self.embedding(x)\n","        b,s,w,e = x.shape\n","        x = x.view(b,s*w,e)\n","        x, __ = self.sent_rnn(x)\n","        \n","        x = x.view(b,s,w,-1)\n","        x = torch.max(x,dim=2)[0]\n","        x = x[:,:,:self.hidden_dim] + x[:,:,self.hidden_dim:]\n","        x = torch.max(x,dim=1)[0]\n","        x = torch.sigmoid(self.l1(F.relu(x)))\n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6cUudHyVF1MJ","colab_type":"text"},"source":["### Training\n","\n","指定使用的運算裝置"]},{"cell_type":"code","metadata":{"id":"zvcLz79bF1MK","colab_type":"code","colab":{}},"source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tZFlIeGqF1MM","colab_type":"text"},"source":["定義一個算分公式, 讓我們在training能快速了解model的效能"]},{"cell_type":"code","metadata":{"id":"ZBwft9lzF1MN","colab_type":"code","colab":{}},"source":["### Helper functions for scoring\n","\n","class F1():\n","    def __init__(self):\n","        self.threshold = 0.5\n","        self.n_precision = 0\n","        self.n_recall = 0\n","        self.n_corrects = 0\n","        self.name = 'F1'\n","\n","    def reset(self):\n","        self.n_precision = 0\n","        self.n_recall = 0\n","        self.n_corrects = 0\n","\n","    def update(self, predicts, groundTruth):\n","        predicts = predicts > self.threshold\n","        self.n_precision += torch.sum(predicts).data.item()\n","        self.n_recall += torch.sum(groundTruth).data.item()\n","        self.n_corrects += torch.sum(groundTruth.type(torch.bool) * predicts).data.item()\n","\n","    def get_score(self):\n","        recall = self.n_corrects / self.n_recall\n","        precision = self.n_corrects / (self.n_precision + 1e-20)\n","        return 2 * (recall * precision) / (recall + precision + 1e-20)\n","\n","    def print_score(self):\n","        score = self.get_score()\n","        return '{:.5f}'.format(score)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"MVAP1jaGF1MP","colab_type":"code","colab":{}},"source":["def _run_epoch(epoch, mode):\n","    \n","    model.train(True)\n","    if mode==\"train\":\n","        description = 'Train'\n","        dataset = trainData\n","        shuffle = True\n","    else:\n","        description = 'Valid'\n","        dataset = validData\n","        shuffle = False\n","    dataloader = DataLoader(dataset=dataset,\n","                            batch_size=batch_size,\n","                            shuffle=shuffle,\n","                            collate_fn=dataset.collate_fn,\n","                            num_workers=8)\n","\n","    trange = tqdm(enumerate(dataloader), total=len(dataloader), desc=description)\n","    loss = 0\n","    f1_score = F1()\n","    for i, (x, y, sent_len) in trange:\n","        o_labels, batch_loss = _run_iter(x,y)\n","        if mode==\"train\":\n","            opt.zero_grad()\n","            batch_loss.backward()\n","            opt.step()\n","\n","        loss += batch_loss.item()\n","        f1_score.update(o_labels.cpu(), y)\n","\n","        trange.set_postfix(\n","            loss=loss / (i + 1), f1=f1_score.print_score())\n","    if mode==\"train\":\n","        history['train'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n","        writer.add_scalar('Loss/train', loss/ len(trange), epoch)\n","        writer.add_scalar('F1_score/train', f1_score.get_score(), epoch)\n","    else:\n","        history['valid'].append({'f1':f1_score.get_score(), 'loss':loss/ len(trange)})\n","        writer.add_scalar('Loss/valid', loss/ len(trange), epoch)\n","        writer.add_scalar('F1_score/valid', f1_score.get_score(), epoch)\n","    trange.close()\n","\n","def _run_iter(x,y):\n","    abstract = x.to(device)\n","    labels = y.to(device)\n","    o_labels = model(abstract).to(device)\n","    l_loss = criteria(o_labels, labels)\n","    return o_labels, l_loss\n","\n","def save(epoch):\n","    if not os.path.exists(os.path.join(CWD,'model')):\n","        os.makedirs(os.path.join(CWD,'model'))\n","    torch.save(model.state_dict(), os.path.join( CWD,'model/model.pkl.'+str(epoch)))\n","    with open(os.path.join( CWD,'model/history.json'), 'w') as f:\n","        json.dump(history, f, indent=4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3-f8J6LHF1MR","colab_type":"code","colab":{}},"source":["model = Net(len(word_dict))\n","\n","opt = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","criteria = torch.nn.BCELoss()\n","model.to(device)\n","history = {'train':[],'valid':[]}\n","\n","## Tensorboard\n","## save path: test_experiment/\n","tf_path = os.path.join(CWD, 'test_experiment')\n","if not os.path.exists(tf_path):\n","    os.mkdir(tf_path)\n","writer = SummaryWriter(os.path.join(tf_path,config_fname))\n","\n","for epoch in range(max_epoch):    \n","    print('Epoch: {}'.format(epoch))\n","    _run_epoch(epoch, 'train')\n","    _run_epoch(epoch, 'valid')\n","    save(epoch)\n","    \n","# Plot the training results    \n","with open(os.path.join(CWD,'model/history.json'), 'r') as f:\n","    history = json.loads(f.read())\n","    \n","train_loss = [l['loss'] for l in history['train']]\n","valid_loss = [l['loss'] for l in history['valid']]\n","train_f1 = [l['f1'] for l in history['train']]\n","valid_f1 = [l['f1'] for l in history['valid']]\n","\n","plt.figure(figsize=(7,5))\n","plt.title('Loss')\n","plt.plot(train_loss, label='train')\n","plt.plot(valid_loss, label='valid')\n","plt.legend()\n","plt.show()\n","\n","plt.figure(figsize=(7,5))\n","plt.title('F1 Score')\n","plt.plot(train_f1, label='train')\n","plt.plot(valid_f1, label='valid')\n","plt.legend()\n","plt.show()\n","\n","print('Best F1 score ', max([[l['f1'], idx] for idx, l in enumerate(history['valid'])]))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cge6re2UF1MT","colab_type":"text"},"source":["### Predict"]},{"cell_type":"code","metadata":{"id":"4oVMRB2qF1MU","colab_type":"code","colab":{}},"source":["### This is the Prediction cell.\n","\n","# fill the epoch of the lowest val_loss to best_model\n","best_model = 2\n","model.load_state_dict(state_dict=torch.load(os.path.join(CWD,'model/model.pkl.{}'.format(best_model))))\n","model.train(False)\n","# double ckeck the best_model_score\n","_run_epoch(1, 'valid')\n","\n","# start testing\n","dataloader = DataLoader(dataset=testData,\n","                            batch_size=32,\n","                            shuffle=False,\n","                            collate_fn=testData.collate_fn,\n","                            num_workers=4)\n","trange = tqdm(enumerate(dataloader), total=len(dataloader), desc='Predict')\n","prediction = []\n","for i, (x,y,_l) in trange:\n","    #x = embedding(x)\n","    o_labels = model(x.to(device))\n","    o_labels = o_labels>0.5\n","    prediction.append(o_labels.to('cpu'))\n","\n","prediction = torch.cat(prediction).detach().numpy().astype(int)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XMwJA-TOF1MW","colab_type":"code","colab":{}},"source":["len(prediction)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"D4sIQ1bXF1MY","colab_type":"code","colab":{}},"source":["def SubmitGenerator(prediction, sampleFile, public=True, filename='prediction.csv'):\n","    \"\"\"\n","    Args:\n","        prediction (numpy array)\n","        sampleFile (str)\n","        public (boolean)\n","        filename (str)\n","    \"\"\"\n","    sample = pd.read_csv(sampleFile)\n","    submit = {}\n","    submit['order_id'] = list(sample.order_id.values)\n","    redundant = len(sample) - prediction.shape[0]\n","    if public:\n","        submit['THEORETICAL'] = list(prediction[:,0]) + [0]*redundant\n","        submit['ENGINEERING'] = list(prediction[:,1]) + [0]*redundant\n","        submit['EMPIRICAL'] = list(prediction[:,2]) + [0]*redundant\n","        submit['OTHERS'] = list(prediction[:,3]) + [0]*redundant\n","    else:\n","        submit['THEORETICAL'] = [0]*redundant + list(prediction[:,0])\n","        submit['ENGINEERING'] = [0]*redundant + list(prediction[:,1])\n","        submit['EMPIRICAL'] = [0]*redundant + list(prediction[:,2])\n","        submit['OTHERS'] = [0]*redundant + list(prediction[:,3])\n","    df = pd.DataFrame.from_dict(submit) \n","    df.to_csv(filename,index=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J6ymnoFFF1Ma","colab_type":"code","colab":{}},"source":["SubmitGenerator(prediction, \n","                os.path.join(CWD,'data/task2_sample_submission.csv'),\n","                True, \n","                os.path.join(CWD,'task2_submission_1112.csv'))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BsT31YAaF1Mc","colab_type":"code","colab":{}},"source":["%load_ext tensorboard"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ha9F_TwyF1Me","colab_type":"code","colab":{}},"source":["%tensorboard --logdir=task2/test_experiment"],"execution_count":0,"outputs":[]}]}